{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMb3y0dNi+BxbHMfahTltRT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steliosg23/Data_Challenge_2025/blob/main/3_dataset_saving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the base directory (adjust this path to your directory)\n",
        "base_dir = '/content/drive/MyDrive/Data Science AUEB/Data Challenge/data/'\n",
        "\n",
        "# Load data\n",
        "data_path = f'{base_dir}expanded_df.csv'  # Replace this with the actual CSV file path\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Preprocess the data\n",
        "df['text'] = df['title_and_description']  # Combining title and description for BERT input\n",
        "df = df[['product_id','text', 'price','neighbor_product_id','class_label']]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load test dataset\n",
        "test_path = '/content/drive/MyDrive/Data Science AUEB/Data Challenge/data/test.txt'\n",
        "test_df = pd.read_csv(test_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJwhIHY4e-0C",
        "outputId": "8f65ae0b-5608-4e18-b7d9-91521ea57a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Convert to Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Tokenize the dataset using multiple processes for parallelism\n",
        "def tokenize_with_progress(dataset, tokenizer_function, num_proc=100):  # num_proc for parallelism\n",
        "    # Manually wrap the map function with tqdm for progress tracking\n",
        "    tokenized_data = []\n",
        "    for example in tqdm(dataset, desc=\"Tokenizing dataset\", total=len(dataset)):\n",
        "        tokenized_data.append(tokenizer_function(example))\n",
        "    return tokenized_data\n",
        "\n",
        "# Tokenize the dataset using multiple processes\n",
        "tokenized_datasets = tokenize_with_progress(dataset, tokenize_function, num_proc=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "5vMb3fz3fAeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: extract tokenized_datasets in pkl and save in same directory\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save tokenized_datasets to a pickle file in the same directory\n",
        "with open(f'{base_dir}tokenized_datasets.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenized_datasets, f)\n"
      ],
      "metadata": {
        "id": "Uwbatv_j6vQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load the pkl file\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the tokenized dataset from the pickle file\n",
        "with open(f'{base_dir}tokenized_datasets.pkl', \"rb\") as f:\n",
        "  tokenized_datasets = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "wzof_plRAKju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "# Update the dataset with tokenized data\n",
        "dataset = dataset.add_column(\"input_ids\", [item[\"input_ids\"] for item in tokenized_datasets])\n",
        "dataset = dataset.add_column(\"attention_mask\", [item[\"attention_mask\"] for item in tokenized_datasets])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pjqdi-f0C0y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert the dataset to a Pandas DataFrame\n",
        "df_dataset = dataset.to_pandas()\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_dataset.to_csv(f'{base_dir}dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "UzqdvgR5ffSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenized_datasets to a pickle file in the same directory\n",
        "with open(f'{base_dir}final_dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(dataset, f)"
      ],
      "metadata": {
        "id": "-WgDEPsigOTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-DKogHGn5Zw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}